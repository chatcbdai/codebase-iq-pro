#!/usr/bin/env python3
"""
Verification script for CodebaseIQ Pro v2.0 implementation
Tests the key changes without requiring full dependencies
"""

import json
import os
from pathlib import Path

def verify_implementation():
    """Verify that all key implementation pieces are in place."""
    
    print("ğŸ” Verifying CodebaseIQ Pro v2.0 Implementation")
    print("=" * 60)
    
    results = []
    
    # 1. Check configuration file
    print("\n1ï¸âƒ£ Checking .claude/config.md...")
    config_path = Path(".claude/config.md")
    if config_path.exists():
        with open(config_path) as f:
            content = f.read()
        if "get_codebase_context" in content and "check_understanding" in content:
            results.append(("âœ…", ".claude/config.md exists with correct instructions"))
        else:
            results.append(("âŒ", ".claude/config.md missing key instructions"))
    else:
        results.append(("âŒ", ".claude/config.md not found"))
    
    # 2. Check server.py modifications
    print("\n2ï¸âƒ£ Checking server.py modifications...")
    server_path = Path("src/codebaseiq/server.py")
    if server_path.exists():
        with open(server_path) as f:
            content = f.read()
        
        # Check for new tools
        new_tools = [
            "get_codebase_context",
            "check_understanding", 
            "get_impact_analysis"
        ]
        
        for tool in new_tools:
            if f'name="{tool}"' in content:
                results.append(("âœ…", f"Tool '{tool}' found in server.py"))
            else:
                results.append(("âŒ", f"Tool '{tool}' NOT found in server.py"))
        
        # Check for implementation methods
        methods = [
            "_get_codebase_context",
            "_check_understanding",
            "_get_impact_analysis"
        ]
        
        for method in methods:
            if f"async def {method}" in content:
                results.append(("âœ…", f"Method '{method}' implemented"))
            else:
                results.append(("âŒ", f"Method '{method}' NOT implemented"))
                
        # Check for persistent storage
        if "~/.codebaseiq" in content and "analysis_cache.json" in content:
            results.append(("âœ…", "Persistent storage implementation found"))
        else:
            results.append(("âŒ", "Persistent storage NOT implemented"))
            
    else:
        results.append(("âŒ", "server.py not found"))
    
    # 3. Check response optimization
    print("\n3ï¸âƒ£ Checking response optimization...")
    if server_path.exists():
        with open(server_path) as f:
            content = f.read()
            
        # Check analyze_codebase returns optimized response
        if "'summary': {" in content and "'next_steps': [" in content:
            results.append(("âœ…", "analyze_codebase returns optimized response"))
        else:
            results.append(("âŒ", "analyze_codebase NOT optimized"))
            
        # Check for token limit handling
        if "_summarize_danger_zones" in content:
            results.append(("âœ…", "Response summarization methods found"))
        else:
            results.append(("âŒ", "Response summarization methods NOT found"))
    
    # 4. Check documentation
    print("\n4ï¸âƒ£ Checking documentation...")
    docs = [
        ("docs/MIGRATION_GUIDE_V2.md", "Migration guide"),
        ("examples/demo_optimized_workflow.py", "Demo script")
    ]
    
    for doc_path, doc_name in docs:
        if Path(doc_path).exists():
            results.append(("âœ…", f"{doc_name} exists"))
        else:
            results.append(("âŒ", f"{doc_name} NOT found"))
    
    # 5. Check test updates
    print("\n5ï¸âƒ£ Checking test updates...")
    test_path = Path("tests/test_full_integration.py")
    if test_path.exists():
        with open(test_path) as f:
            content = f.read()
        
        if "test_get_codebase_context" in content:
            results.append(("âœ…", "New tool tests added"))
        else:
            results.append(("âŒ", "New tool tests NOT added"))
            
        if "v2.0" in content:
            results.append(("âœ…", "Tests updated for v2.0"))
        else:
            results.append(("âŒ", "Tests NOT updated for v2.0"))
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š VERIFICATION RESULTS:")
    print("=" * 60)
    
    passed = 0
    failed = 0
    
    for status, message in results:
        print(f"{status} {message}")
        if status == "âœ…":
            passed += 1
        else:
            failed += 1
    
    print("\n" + "-" * 60)
    print(f"Total checks: {len(results)}")
    print(f"âœ… Passed: {passed}")
    print(f"âŒ Failed: {failed}")
    
    if failed == 0:
        print("\nğŸ‰ ALL CHECKS PASSED! Implementation is complete.")
    else:
        print(f"\nâš ï¸  {failed} checks failed. Please review the implementation.")
        
    # Additional verification - simulate key functionality
    print("\n" + "=" * 60)
    print("ğŸ§ª SIMULATING KEY FUNCTIONALITY:")
    print("=" * 60)
    
    # Simulate token size check
    print("\nğŸ“ Token Size Simulation:")
    sample_response = {
        "instant_context": "Sample context" * 100,
        "danger_zones": {"summary": "42 files need caution"},
        "critical_files": ["file1.py", "file2.py", "file3.py"],
        "metadata": {"files_analyzed": 347}
    }
    
    json_str = json.dumps(sample_response)
    estimated_tokens = len(json_str) / 4
    print(f"Sample response size: {len(json_str)} chars")
    print(f"Estimated tokens: {estimated_tokens:.0f}")
    print(f"Under 25K limit: {'âœ… Yes' if estimated_tokens < 25000 else 'âŒ No'}")
    
    # Check understanding scoring simulation
    print("\nğŸ¯ Understanding Score Simulation:")
    score = 0
    max_score = 10
    
    # Simulate scoring logic
    if True:  # Clear plan
        score += 2
    if True:  # Files identified
        score += 2
    if True:  # Understanding demonstrated
        score += 3
    if True:  # Business impact
        score += 2
    if True:  # Testing plan
        score += 1
        
    print(f"Simulated score: {score}/{max_score}")
    print(f"Approval (>= 8): {'âœ… Yes' if score >= 8 else 'âŒ No'}")
    
    return passed, failed

if __name__ == "__main__":
    verify_implementation()